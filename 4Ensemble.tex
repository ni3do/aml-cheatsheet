% -*- root: Main.tex -*-
\section{Ensemble method}

\textbf{Classif. Trees}: shallow $\rightarrow$ bias$\uparrow$, variance$\downarrow$, deep $\rightarrow$ bias$\downarrow$, variance$\uparrow$

\textbf{Bagging}: train \textit{simple} classifier on different data sets $\Rightarrow$ variances$\downarrow$, covar.$\downarrow$ and biases$\downarrow$ \\
$\hat{b}(x) = \left\{ \begin{array}{ll} \frac{1}{M}\sum_{t \leq M} b^{(t)}(x) & \mathrm{regression} \\ \mathrm{vote}\{ b^{(1)}, \cdots, b^{(M)} \} & \mathrm{classification}   \end{array} \right.$

\textbf{Boosting}: adaptive combination of poor learners with sufficient diversity $\Rightarrow$ var.$\downarrow$, biases$\downarrow$

\begin{comment}
\subsection*{Random Forest}
for b=1:B do:\\
draw a bootstrap sample $D_b$\\
repeat until node size<$n_{min}$:\\
1. select $m$ features from $p$ features\\
2. pick the best variable and split-point\\
3. Split the node accordingly\\
return the forest $\{\hat{c}_b(x)\}_{b=1}^B$
\end{comment}

%Boosting: Train weak learners sequentially on all data, but reweight misclassifed samples higher, Bias $\downarrow$
\subsection*{Adaboost}
Initialize weights $w_i = 1/n$, for $b = 1,...,B$ do: \\
1. Fit classifier $c_b(x)$ with weights $w_i$ \\
2. Compute error $\epsilon_b = \sum_i w_i^{(b)} \mathbbm{1}_{[c_b(x_i) \not = y_i]} / \sum_i w_i^{(b)}$\\
3. Compute coeff. $\alpha_b = log(\frac{1-\epsilon_b}{\epsilon_b})$\\
4. Update weights $w_i = w_i \exp(\alpha_b \mathbbm{1}_{[y_i \not = c_b(x_i)]})$
Return $\hat{c}_B(x) = \text{sign} \left ( \sum_{b=1}^B \alpha_b c_b(x) \right )$\\

\textbf{Loss}: Exponential loss function\\
\textbf{Model}: Additive logistic regression\\
Bayesian approach (assumes posteriors),
Newton-like updates (Gradient Descent)

%\newpage

%\subsection*{Bagging}
%\textbf{for} $b=1$ to $B$ \textbf{do}:\\
%1. $Z^{*b}=$ b-th bootstrap sample from Z\\
%2. Construct classifier $c_b$ based on $Z^{*b}$\\
%\textbf{return} ensemble class. $\hat{c}_B(x)=sgn(\sum_{i=1}^{B} c_i(x))$\\
%\textbf{Works}: Covariance small (different subset for training), Variance small (similar behaviour of weak learners), biases weakly affected.\\
%\textbf{Bag. aggr. pred.}: $h_B(x)=E_{D'\sim D}[h_{D'}(x)]$\\
%\textbf{Ideal aggr. pred.}: $h_A(x)=E_{D\sim P(x,y)}[h_D(x)]$\\
%$E_D[L(y,h_D(x))]=E_D[(y-h_D(x))^2]=E_D[y^2]-2E_D[y\cdot h_D(x)]+E_D[h_D(x)^2]=y^2-2y\cdot E_D[h_D(x)]+E_D[h_D(x)^2]\geq y^2-2y\cdot E_D[h_D(x)]+E_D[h_D(x)]^2=y^2-2y\cdot h_A(x)+h_A(x)^2=(y-h_A(x))^2=L(y,h_A(x))$\\
%\textbf{Bias$\downarrow$\&Var.$\downarrow$}: Use complex decision tree (bias$\downarrow$), ensemble mult. decision trees (var$\downarrow$)
